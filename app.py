import streamlit as st 
from transformers import pipeline #

# @st.cache_resource #
# ุชู ุฅุฒุงูุฉ @st.cache_resource ูุคูุชุงู ูุชุฌูุจ ุงููุดุงููุ ููููู ุฅุถุงูุชู ูุงุญูุงู
def load_model(): #
    # ูุชุญููู Master Grand - ุงุณุชุฎุฏุงู ูููุฐุฌ GPT-2 ูุชูููุฏ ุงููุตูุต
    return pipeline("text-generation", model="gpt2") 

# ูุฌุจ ุฃู ูุชุฃูุฏ Streamlit ูู ุชุซุจูุช ุงูููุชุจุงุช (transformers, torch) ูู requirements.txt

chatbot_pipeline = load_model() #

# ูุงุฌูุฉ ุงููุณุชุฎุฏู ูุงูุชุตููู
st.set_page_config(page_title="ุงูุฃุณุทูุฑุฉ", layout="wide") #
st.title("๐งโโ๏ธ ูููุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุชูููุฏ ุงููุตูุต") #

# ุตูุฏูู ุฅุฏุฎุงู ุงููุต
prompt = st.text_area("ุฃุฏุฎู ุงููุต ุงูุฃููู (Prompt) ููุง:", "ุฑุญูุฉ ุงููุงุณุชุฑ ุจุฏุฃุช ุจู")

if st.button("ุชูููุฏ ุงููุต"):
    if prompt:
        with st.spinner("ุฌุงุฑู ุชูููุฏ ุงููุต... ูุฏ ูุณุชุบุฑู ุงูุฃูุฑ ุจุนุถ ุงูููุช..."):
            # ุชูููุฏ ุงููุต ุจุงุณุชุฎุฏุงู ุงููููุฐุฌ
            result = chatbot_pipeline(prompt, max_length=150, num_return_sequences=1)
            
            # ุนุฑุถ ุงููุชูุฌุฉ
            st.subheader("ุงููุชูุฌุฉ:")
            st.code(result[0]['generated_text'])
    else:
        st.error("ุงูุฑุฌุงุก ุฅุฏุฎุงู ูุต ุฃููู ูุจุฏุก ุงูุชูููุฏ.")
 

